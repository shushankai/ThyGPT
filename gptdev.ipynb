{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf49866",
   "metadata": {},
   "source": [
    "# ThyGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb0251",
   "metadata": {},
   "source": [
    "A char level transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b17b86",
   "metadata": {},
   "source": [
    "## Help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3f122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6311ac1e-e75a-45e5-a9d4-75c7f78f8445",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47edebc-21a5-437a-8829-b61ad0d188ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  1806k      0 --:--:-- --:--:-- --:--:-- 1806k\n"
     ]
    }
   ],
   "source": [
    "# downloading dataset\n",
    "!curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2c1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b9c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6b4bec-b9dd-4375-ad93-f67d25eac3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1656c9b-ce3a-4fd5-91d6-3d793852b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# creating the vocab\n",
    "chars = sorted(set(\"\".join(text)))\n",
    "vocab = len(chars)\n",
    "print(vocab)\n",
    "print(\"\".join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7aac476-9b6c-40e7-b55e-79b17f4b4c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# Creating a maping from chars to integer\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "\n",
    "# Encoder decoder\n",
    "encode = lambda s: [\n",
    "    stoi[c] for c in s\n",
    "]  # takes a string and convert it to a list of number\n",
    "decode = lambda l: \"\".join(\n",
    "    itos[i] for i in l\n",
    ")  # takes a list of number and convert it to a string\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84261ad1-3ff3-4923-bb20-70f735720cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the compelete dataset\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df74acf2-f4ba-4d29-b83c-547fbc71a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting dataset into train/val\n",
    "split_index = int(0.9 * len(data))\n",
    "train = data[:split_index]\n",
    "val = data[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f684d5-7ad2-4fef-90fe-da455824379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3fedeac-9dc6-49e1-9741-101480963fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8  # size of chunk's\n",
    "train[: block_size + 1]\n",
    "# here there is quite a lot of information packed together\n",
    "# 18 is followed by 47 , 18, 47 is followed by 56 .....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fa1f35-11d6-4e36-9685-db73160f3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input tensor is tensor([18]) then target tensor is: 47\n",
      "When input tensor is tensor([18, 47]) then target tensor is: 56\n",
      "When input tensor is tensor([18, 47, 56]) then target tensor is: 57\n",
      "When input tensor is tensor([18, 47, 56, 57]) then target tensor is: 58\n",
      "When input tensor is tensor([18, 47, 56, 57, 58]) then target tensor is: 1\n",
      "When input tensor is tensor([18, 47, 56, 57, 58,  1]) then target tensor is: 15\n",
      "When input tensor is tensor([18, 47, 56, 57, 58,  1, 15]) then target tensor is: 47\n",
      "When input tensor is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target tensor is: 58\n"
     ]
    }
   ],
   "source": [
    "X = train[:block_size]\n",
    "y = train[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = X[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"When input tensor is {context} then target tensor is: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ea6f4c-3cbe-421f-b4fb-e20fa22f267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to give data in batch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split: str, batch_size: int = 4, block_size: int = 8):\n",
    "    data = train if split == \"train\" else val\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    X = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "702b968b-789d-43ba-a860-819d14033b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "BLOCK_SIZE = 8\n",
    "SEED = 1337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df5f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "Our input features: torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "Target Values: torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(split=\"train\", batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)\n",
    "print(\"________\")\n",
    "print(f\"Our input features: {xb.shape}\\n{xb}\")  # this is our input to the transformers\n",
    "print(f\"Target Values: {yb.shape}\\n{yb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d76e89-56df-4d6d-9b5a-3fd0872b9241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "torch.Size([1, 101])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# Creating the bigram language model\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BiagramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding(x)  # b, T, C [Batch, Token, Channel]\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            target = target.view(B * T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # When we are doing the forward pass we are getting the shape as [batch_size, token_size, channels]\n",
    "    # because what we are doing is getting the values from the lookup table of size[65, 65]\n",
    "    # For each token, you’re predicting logits over 65 possible tokens.\n",
    "    # so for ex if our token value is 34 then we will get the complete values along the row at index 34\n",
    "\n",
    "    def generate(self, idx, max_length):\n",
    "        for _ in range(max_length):\n",
    "            # pass the index to the model to get the logits\n",
    "            # the shape of the logits will be [batch_size, Token_size, Channels]\n",
    "            # for the first time the shape of idx will be [1,1] which when passed through the\n",
    "            # forward pass will gives us [1,1,65]\n",
    "\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # Now we will reshape the logits for calculating the probabilities\n",
    "            # now the shape of logits will be [batch_size * token_size, channels]\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            prob = torch.softmax(logits, dim=-1)  # B*T, C\n",
    "            new_idx = torch.multinomial(input=prob, num_samples=1)  # B, 1\n",
    "            idx = torch.cat((idx, new_idx), dim=1)  # B, T + 1\n",
    "        return idx\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "m = BiagramLanguageModel(vocab_size=vocab)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "\n",
    "new_preds = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_length=100)\n",
    "print(new_preds.shape)\n",
    "print(decode(new_preds[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b9e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=m.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84aaabff-bd53-4773-a802-dd57e5e6b2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4774329662323\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "BATCH_SIZE = 32\n",
    "torch.manual_seed(SEED)\n",
    "for step in range(10000):\n",
    "    m.train()\n",
    "    xb, yb = get_batch(\"train\", BATCH_SIZE, block_size)\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25460d4-9726-4f43-a683-7183d929c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ope llled ilok d avetiqu oulartouef this ain wircewe ore; my\n",
      "We k woTiar'll wan, OFFon d y fthen ixpeyere we tus!\n",
      "msetpecondall alll casove; t bre mit ar. thanouig!\n",
      "\n",
      "T: at ldaysicome?\n",
      "NTh haveo and ink! I sul Bed hernong! KRTI E:\n",
      "Fot d mome garesth oust he ovee perrear hanorerin gew,\n",
      "Gost;\n",
      "MEMe thous mace ffean ck trboeds y hadowenarng pongoxisiche s the nd cke fowe fo ES:\n",
      "S iss o:\n",
      "DO,\n",
      "Whas GHUS:\n",
      "Fofours DOLAGofthiowit wit inenqu anary te selee enges ke tha in spe yor s 's lo st ICRKE:\n",
      "'s\n",
      "Wheyer\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode(\n",
    "        m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_length=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "# All gi-brish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f636e8",
   "metadata": {},
   "source": [
    "> 💡 NOTE:  \n",
    "> **When performing operations that involve the dim parameter (like sum), it’s crucial to understand how it affects the tensor dimensions.**  \n",
    "> **Suppose we have a tensor of shape [2, 2, 2] — that is, 2 matrices, each of shape [2, 2].**  \n",
    "> **• Summing along dim=0 means adding corresponding elements across the 2 matrices. The batch dimension is reduced, so the result has shape [2, 2].**  \n",
    "> **• Summing along dim=1 means summing the rows within each matrix. Each matrix’s rows are combined, reducing the row dimension. The resulting shape is [2, 1, 2].**  \n",
    "> **• Summing along dim=2 means summing the columns within each row of each matrix, reducing the column dimension. The result will have shape [2, 2, 1].**\n",
    ">\n",
    "> **In short, dim=n collapses that specific axis by applying the operation along it, reducing its size.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453e441",
   "metadata": {},
   "source": [
    "## Self Attention Mathematical Trick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9eee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Embeddings (bow[0]):\n",
      "tensor([[ 0.7352, -0.9397],\n",
      "        [ 0.5135,  0.5951],\n",
      "        [-1.0797, -0.9238],\n",
      "        [ 1.1126, -0.0741],\n",
      "        [ 0.1068, -0.0124],\n",
      "        [-0.6453, -1.7736],\n",
      "        [ 1.0020,  0.4353],\n",
      "        [-0.8329,  0.4699]])\n",
      "Context-aware Averaged Embeddings (xbow[0]):\n",
      "tensor([[ 0.7352, -0.9397],\n",
      "        [ 0.6243, -0.1723],\n",
      "        [ 0.0563, -0.4228],\n",
      "        [ 0.3204, -0.3356],\n",
      "        [ 0.2777, -0.2710],\n",
      "        [ 0.1238, -0.5214],\n",
      "        [ 0.2493, -0.3847],\n",
      "        [ 0.1140, -0.2779]])\n"
     ]
    }
   ],
   "source": [
    "# Approach 1:\n",
    "# The main objective is to ensure that the model only attends to **past** and **current** tokens — not future ones.\n",
    "# For example, in the sequence [1, 2, 3, 4, 5, 6, 7, 8], when predicting token 3,\n",
    "# the model should only have access to tokens 1 and 2 (not 4–8).\n",
    "\n",
    "# One simple way to incorporate past information is by averaging the embeddings of previous tokens\n",
    "# For example:\n",
    "#   [1]         → [1]\n",
    "#   [1, 2]      → [(1+2)/2]\n",
    "#   [1, 2, 3]   → [(1+2+3)/3]\n",
    "# This allows the model to have a context-aware representation at each position.\n",
    "\n",
    "B, T, C = 4, 8, 2  # B: batch size, T: sequence length, C: embedding dimension\n",
    "\n",
    "# Randomly initialize a batch of token embeddings (e.g., from an embedding layer)\n",
    "bow = torch.randn((B, T, C))  # \"bag-of-words\" embeddings\n",
    "\n",
    "# Create an empty tensor to store the averaged representations\n",
    "xbow = torch.zeros_like(bow)\n",
    "\n",
    "print(\"Original Embeddings (bow[0]):\")\n",
    "print(bow[0])\n",
    "\n",
    "# For each batch and each time step, compute the mean of all previous (and current) embeddings\n",
    "for b in range(B):  # Loop over each batch\n",
    "    for t in range(T):  # Loop over each time step\n",
    "        xprev = bow[b, : t + 1]  # Get all previous + current token embeddings\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)  # Average them and assign to xbow\n",
    "\n",
    "print(\"Context-aware Averaged Embeddings (xbow[0]):\")\n",
    "print(xbow[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04d601ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Approach 2:\n",
    "# Create a lower triangular matrix of shape [T, T]\n",
    "# This ensures that each position only has access to itself and the tokens before it\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "# Normalize each row so the sum equals 1\n",
    "# This gives us a simple averaging mask, where past tokens are averaged equally\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# Perform matrix multiplication with the bag-of-words (bow) embeddings\n",
    "# This applies the weighted average of past token embeddings for each position\n",
    "xbow_fast = wei @ bow\n",
    "\n",
    "# Verify that the faster implementation produces the same result as the previous one\n",
    "print(\n",
    "    torch.allclose(xbow_fast, xbow)\n",
    ")  # Should return True if both methods are equivalent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f94160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Approach 3:\n",
    "# In Approach 3 we will be using softmax and converting the 0 in the lower triangular matrix to inf reason being\n",
    "# right now we are initializing the wei with zeros but what these values are in actual is they are sort of interaction\n",
    "# strength and in actual they will have some values\n",
    "# we don't want the future wei to interact with our current token\n",
    "\n",
    "# Creating a lower triangular matrix of shape[T,T]\n",
    "# This ensures us that each position has access to position before it not the future\n",
    "trill = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "# Creating a wei[INteraction strength matrix] of zeros shape[T,T]\n",
    "wei = torch.zeros((T, T))\n",
    "\n",
    "# Masking the wei matrix similar to trill matrix and updating the values of zeros to -inf\n",
    "# This tell us that absolutely in no case that the present position should interact with future position\n",
    "wei = wei.masked_fill(trill == 0, float(\"-inf\"))\n",
    "\n",
    "# Doing softmax to normalize our values in wei\n",
    "wei = torch.softmax(wei, dim=-1)\n",
    "\n",
    "# Matrix multiplication to average the each position with its previous position so that\n",
    "# each position has some level of info about the previous positions\n",
    "wei = wei @ bow\n",
    "\n",
    "# Checking\n",
    "print(torch.allclose(wei, xbow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "816d05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b14955a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention (single head)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Batch_size, Time, Channel \n",
    "B, T, C = 4, 8, 32\n",
    "\n",
    "# Input Tensor \n",
    "bow = torch.randn((B, T, C))                              # Shape: (4 8, 32) 4 matrixs of size 8(chars) represented by 32 numbers\n",
    "\n",
    "# Single Head Size \n",
    "head_size = 16\n",
    "\n",
    "# Linear layer having input features 32, and out features 16\n",
    "key = nn.Linear(C, head_size, bias= False)             \n",
    "query = nn.Linear(C, head_size, bias= False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# Passing the input through the linear layer \n",
    "k = key(bow)                                             # Shape: (4, 8, 16) \n",
    "q = query(bow)                                           # Shape: (4, 8, 16)\n",
    "v = value(bow)\n",
    "# Now the wei - which is our matrix that tell us how much one token is related to other is dependend on the input \n",
    "wei = k @ q.transpose(-2, -1)                            # Shape (4, 8, 16) @ (4, 16, 8) -> (4, 8, 8)\n",
    "# The wei matrix now contains informations about how each token in the context is how strongly related to one another \n",
    "\n",
    "\n",
    "# Now we need to make sure that each position has access to previous token not the future \n",
    "# For this we will mask and normallize the wei \n",
    "trill = torch.tril(torch.ones(T, T))\n",
    "\n",
    "wei = wei.masked_fill(trill == 0, float(\"-inf\"))\n",
    "\n",
    "# Normalizing using the softmax \n",
    "wei = torch.softmax(wei, dim=1)\n",
    "out = wei @ v                                          # Shape (4, 8, 8) @ (4, 8, 32) -> (4, 8, 32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92612997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d95b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387f3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73e6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebb43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12876cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77af59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28362031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fec9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1148ba-8f9f-4c26-bf89-c99bcac36403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92732327-6860-4f3d-9a80-7d65ed444961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thygpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
